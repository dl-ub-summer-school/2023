{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dl-ub-summer-school/2023/blob/main/DLUB2023_language_model_solution.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup"
      ],
      "metadata": {
        "id": "6JWbbQronDoE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os"
      ],
      "metadata": {
        "id": "YD8_SXvDIRBT"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/dl-ub-summer-school/2023.git"
      ],
      "metadata": {
        "id": "KiCfJJ9u2E9e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "167dc618-0e88-4fbe-96a1-ce4d285cc169"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into '2023'...\n",
            "remote: Enumerating objects: 84, done.\u001b[K\n",
            "remote: Counting objects: 100% (84/84), done.\u001b[K\n",
            "remote: Compressing objects: 100% (69/69), done.\u001b[K\n",
            "remote: Total 84 (delta 25), reused 57 (delta 11), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (84/84), 10.41 MiB | 5.37 MiB/s, done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.chdir('2023/seminar_transformers/src')"
      ],
      "metadata": {
        "id": "0PRLakRYIrs8"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import math\n",
        "import utils\n",
        "import dataloader\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.optim import Adam"
      ],
      "metadata": {
        "id": "Pgt1gM4F-Zay"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.chdir('../../../')"
      ],
      "metadata": {
        "id": "OqDLpRjNL97z"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.listdir('./')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zQ1E8Gu6L_Ni",
        "outputId": "60d4a787-73da-48ef-ec21-2a6d4bdfa4eb"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['.config', '2023', 'sample_data']"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Below all hyper parameters assigned for the\n",
        "# convenience of notebook, however, it is not a best practice\n",
        "# as you should probably want to use hydra or argparse instead\n",
        "gpu_id=0\n",
        "seed=42\n",
        "save=0\n",
        "\n",
        "# device = torch.device(f\"cuda:{gpu_id}\" if torch.cuda.is_available() else 'cpu')\n",
        "device = torch.device('cpu')\n",
        "print(device)\n",
        "\n",
        "num_dec_layers=6\n",
        "max_len=20\n",
        "model_dim=512\n",
        "hidden_size=2048\n",
        "d_k=64\n",
        "d_v=64\n",
        "n_head=8\n",
        "d_prob=0.1\n",
        "max_norm=5.0\n",
        "\n",
        "# MASKED_VALUE = -1e9\n",
        "MASKED_VALUE = float('-inf')\n",
        "\n",
        "# attention is all you need hyper-params\n",
        "n_epochs=100\n",
        "batch_size=128\n",
        "lr=1e-3\n",
        "beta1=0.9\n",
        "beta2=0.98\n",
        "eps=1e-9\n",
        "weight_decay=1e-3\n",
        "# bluescore\n",
        "k=4"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m2Rwka0W-b4O",
        "outputId": "d5b9b5ca-eda2-429c-f8cd-00e9c2f38e44"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Implementing Transformer from scratch"
      ],
      "metadata": {
        "id": "6rGsONQIyxyL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "dataloader and dataset_prep.py contents"
      ],
      "metadata": {
        "id": "SwBJcov2qN4c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.2 Embedding layer"
      ],
      "metadata": {
        "id": "LJJ5r3MT3fQg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.4 Implementation"
      ],
      "metadata": {
        "id": "2YNCl6yX-Swa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Embedding_Layer(nn.Module):\n",
        "\n",
        "    def __init__(self, num_token, dim_model, max_seq_len, d_prob):\n",
        "        \"\"\"Implementation of embedding and positional embeddings.\n",
        "\n",
        "        Args:\n",
        "            num_token (int): Number of token\n",
        "            dim_model (_type_): Dimension of the model\n",
        "            max_seq_len (_type_): Maximum sequence length\n",
        "            d_prob (_type_): Dropout probability\n",
        "        \"\"\"\n",
        "        super(Embedding_Layer, self).__init__()\n",
        "        self.num_token = num_token\n",
        "        self.dim_model = dim_model\n",
        "        self.max_seq_len = max_seq_len\n",
        "        self.d_prob = d_prob\n",
        "        self.emb = nn.Embedding(num_token, dim_model)\n",
        "        self.drop_out = nn.Dropout(d_prob)\n",
        "        self.pos_enc = torch.zeros((self.max_seq_len, self.dim_model))\n",
        "        for pos in range(self.max_seq_len):\n",
        "            for idx in range(0, self.dim_model, 2):\n",
        "                self.pos_enc[pos, idx] = torch.sin(torch.tensor(pos / (10000.0) ** (float(idx) / self.dim_model)))\n",
        "                self.pos_enc[pos, idx + 1] = torch.cos(torch.tensor(pos / (10000.0) ** (float(idx) / self.dim_model)))\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.emb(x)\n",
        "        x = x + self.pos_enc[:x.size(1)]\n",
        "        x = self.drop_out(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "RwtbqV7By8QY"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ### Test it out\n",
        "# emb_test = Embedding_Layer(30, 10, 4, 0.1)\n",
        "# input_test = torch.LongTensor([[1,2,4,5],[4,3,2,9]])\n",
        "# emb_test(input_test).size()"
      ],
      "metadata": {
        "id": "3DbWSipMBeJ9"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Transformer Decoder"
      ],
      "metadata": {
        "id": "BTRq8N6rXui2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.1 Scaled Dot-Product Attention (Masked)"
      ],
      "metadata": {
        "id": "KFmGJo9SXz4k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MaskedScaledDotProductAttention(nn.Module):\n",
        "    def __init__(self, d_k, d_prob):\n",
        "        super(MaskedScaledDotProductAttention, self).__init__()\n",
        "        self.d_k = d_k\n",
        "        self.softmax = nn.Softmax(dim=-1)\n",
        "        self.drop_out = nn.Dropout(d_prob)\n",
        "\n",
        "    def forward(self, x_q, x_k, x_v):\n",
        "        dot_product = torch.matmul(x_q, x_k.transpose(-1, -2))\n",
        "        scaled_dot_product = dot_product / math.sqrt(self.d_k)\n",
        "        true_arr = torch.ones_like(scaled_dot_product)\n",
        "        mask = torch.tril(true_arr).bool()\n",
        "        scaled_dot_product = scaled_dot_product.masked_fill(mask==False, MASKED_VALUE)\n",
        "        reg_scaled_dot_product = self.softmax(scaled_dot_product)\n",
        "        reg_scaled_dot_product = self.drop_out(reg_scaled_dot_product)\n",
        "        scaled_dot_product_attn = torch.matmul(reg_scaled_dot_product, x_v)\n",
        "        return scaled_dot_product_attn"
      ],
      "metadata": {
        "id": "_5KMSXr5Xl6Z"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.randn(1, 5, 3)\n",
        "masked_attn = MaskedScaledDotProductAttention(3, 0)\n",
        "print(x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1-ZopzfHkpt0",
        "outputId": "2de45386-9fae-499a-c68d-cb0e2f5c97f9"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[ 0.8741,  0.4530, -0.3194],\n",
            "         [ 1.7523, -0.4448,  0.0122],\n",
            "         [-0.7552, -0.8099,  0.4971],\n",
            "         [-1.3694,  0.1062,  0.0373],\n",
            "         [ 1.8494, -0.2583, -1.1109]]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "masked_attn = MaskedScaledDotProductAttention(3, 0)\n",
        "masked_attn(x, x, x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CJXKju8clvm2",
        "outputId": "da93193d-2ef7-4f4c-87d6-4d0c4c9c5624"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[ 0.8741,  0.4530, -0.3194],\n",
              "         [ 1.5365, -0.2242, -0.0692],\n",
              "         [-0.0932, -0.5623,  0.2951],\n",
              "         [-0.8251, -0.1781,  0.1495],\n",
              "         [ 1.6474, -0.2322, -0.6866]]])"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "true_arr = torch.ones(5,5)\n",
        "mask = torch.tril(true_arr).bool()\n",
        "print(mask)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OqpWeQWQbWhW",
        "outputId": "e32d0e91-94e3-451b-a0f8-a864e1fa422f"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ True, False, False, False, False],\n",
            "        [ True,  True, False, False, False],\n",
            "        [ True,  True,  True, False, False],\n",
            "        [ True,  True,  True,  True, False],\n",
            "        [ True,  True,  True,  True,  True]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.2 Masked Multi-Head Attention"
      ],
      "metadata": {
        "id": "MH9hy7X4X-7V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MaskedMultiHeadAttention(nn.Module):\n",
        "    def __init__(self, dim_model, d_k, d_v, n_head, d_prob):\n",
        "        super(MaskedMultiHeadAttention, self).__init__()\n",
        "        self.dim_model = dim_model\n",
        "        self.d_k = d_k\n",
        "        self.d_v = d_v\n",
        "        self.n_head = n_head\n",
        "\n",
        "        self.w_q = nn.Linear(dim_model, n_head * d_k)\n",
        "        self.w_k = nn.Linear(dim_model, n_head * d_k)\n",
        "        self.w_v = nn.Linear(dim_model, n_head * d_v)\n",
        "        self.w_o = nn.Linear(n_head * d_v, dim_model)\n",
        "\n",
        "        self.masked_scaled_dot_prod = MaskedScaledDotProductAttention(d_k, d_prob)\n",
        "\n",
        "    def forward(self, q, k, v):\n",
        "        x_q = self.w_q(q).view(len(q), -1, self.n_head, self.d_k).transpose(1, 2)\n",
        "        x_k = self.w_k(k).view(len(k), -1, self.n_head, self.d_k).transpose(1, 2)\n",
        "        x_v = self.w_v(v).view(len(v), -1, self.n_head, self.d_v).transpose(1, 2)\n",
        "        scaled_dot_product_attn = self.masked_scaled_dot_prod(x_q, x_k, x_v)\n",
        "        scaled_dot_product_attn = scaled_dot_product_attn.transpose(1, 2)\n",
        "        scaled_dot_product_attn = scaled_dot_product_attn.reshape(len(v), -1, self.d_v * self.n_head)\n",
        "        # return scaled_dot_product_attn.shape, scaled_dot_product_attn\n",
        "        output = self.w_o(scaled_dot_product_attn)\n",
        "        return output"
      ],
      "metadata": {
        "id": "MeT5HdJNYDgR"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.rand(2,3)\n",
        "print(x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j8TdKKtUtSMN",
        "outputId": "d9ec345a-c4cc-4c3b-ea4e-220da7900699"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.3526, 0.3387, 0.2679],\n",
            "        [0.0920, 0.1494, 0.5228]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y = x"
      ],
      "metadata": {
        "id": "8_03AjRqtXyp"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x += 10\n",
        "print(y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mZWScJaOt7dJ",
        "outputId": "d02ffeea-81a9-4e4e-ea60-398428140052"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[10.3526, 10.3387, 10.2679],\n",
            "        [10.0920, 10.1494, 10.5228]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.rand(2,3)\n",
        "print(y,x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m8Mat75Btdgl",
        "outputId": "39e32de3-5637-416b-92f7-8485883645d1"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[10.3526, 10.3387, 10.2679],\n",
            "        [10.0920, 10.1494, 10.5228]]) tensor([[0.9076, 0.7688, 0.7885],\n",
            "        [0.4237, 0.1943, 0.4677]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mod = MaskedMultiHeadAttention(4, 2, 2, 2, 0)\n",
        "x = torch.randn(1, 5, 4)\n",
        "print(x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UvCsIRCWpJci",
        "outputId": "f96166e1-2a68-4a50-9aef-6f1ebe0f5b7b"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[-0.8556, -0.7234, -1.3710, -0.7861],\n",
            "         [-1.5740,  0.0348,  0.0625, -0.6335],\n",
            "         [-0.2133,  0.2485,  0.8819, -0.8583],\n",
            "         [ 0.8979,  1.4713, -0.7775, -1.2555],\n",
            "         [-1.4898, -3.2574,  0.8289,  0.0620]]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mod(x, x ,x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K3u-XqIZpYIO",
        "outputId": "9693058b-f6f1-4aa4-88ff-3a62ec1e4707"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[ 0.2594, -0.2722, -0.8489,  0.2664],\n",
              "         [ 0.1221, -0.1657, -0.6419,  0.3495],\n",
              "         [ 0.0653, -0.2347, -0.3988,  0.4096],\n",
              "         [ 0.0837, -0.4569, -0.1734,  0.4193],\n",
              "         [ 0.0174, -0.1026, -0.3378,  0.3357]]], grad_fn=<ViewBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class FFNN(nn.Module):\n",
        "    def __init__(self, dim_model, dim_hidden, d_prob):\n",
        "        super(FFNN, self).__init__()\n",
        "        self.fc1 = nn.Linear(dim_model, dim_hidden)\n",
        "        self.fc2 = nn.Linear(dim_hidden, dim_model)\n",
        "\n",
        "        self.relu = nn.ReLU()\n",
        "        self.drop_out = nn.Dropout(d_prob)\n",
        "\n",
        "    def forward(self, x):\n",
        "        output = self.fc2(self.drop_out(self.relu(self.fc1(x))))\n",
        "        return output"
      ],
      "metadata": {
        "id": "1qH5cHapg-tl"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.3 Decoder Layer"
      ],
      "metadata": {
        "id": "oBxKgB4rYHGH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderLayer(nn.Module):\n",
        "    def __init__(self, dim_model, d_k, d_v, n_head, dim_hidden, d_prob):\n",
        "        super(DecoderLayer, self).__init__()\n",
        "        self.dim_model = dim_model\n",
        "        self.d_k = d_k\n",
        "        self.d_v = d_v\n",
        "        self.n_head = n_head\n",
        "        self.dim_hidden = dim_hidden\n",
        "        self.d_prob = d_prob\n",
        "\n",
        "        self.masked_multi_head_attention = MaskedMultiHeadAttention(dim_model, d_k, d_v, n_head, d_prob)\n",
        "        self.ffnn = FFNN(dim_model, dim_hidden, d_prob)\n",
        "\n",
        "        self.layer_norm1 = nn.LayerNorm(dim_model)\n",
        "        self.layer_norm2 = nn.LayerNorm(dim_model)\n",
        "\n",
        "        self.drop_out = nn.Dropout(d_prob)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x_residual = x\n",
        "        x = self.masked_multi_head_attention(x, x, x)\n",
        "        x = self.layer_norm1(x + x_residual)\n",
        "        x_residual = x\n",
        "        x = self.ffnn(x)\n",
        "        x = self.layer_norm2(x + x_residual)\n",
        "        return x"
      ],
      "metadata": {
        "id": "aPCVUuINYJif"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.4 Decoder"
      ],
      "metadata": {
        "id": "NJse23TaYOa8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self, dim_model, d_k, d_v, n_head, dim_hidden, d_prob, n_dec_layer):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.dim_model = dim_model\n",
        "        self.d_k = d_k\n",
        "        self.d_v = d_v\n",
        "        self.n_head = n_head\n",
        "        self.dim_hidden = dim_hidden\n",
        "        self.d_prob = d_prob\n",
        "        self.n_dec_layer = n_dec_layer\n",
        "\n",
        "        self.dec_layers = nn.ModuleList([DecoderLayer(dim_model, d_k, d_v, n_head, dim_hidden, d_prob) for _ in range(n_dec_layer)])\n",
        "\n",
        "    def forward(self, x):\n",
        "        for layer in self.dec_layers:\n",
        "            x = layer(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "Q7_ZZrAXYQI5"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Transformer"
      ],
      "metadata": {
        "id": "fVLiXpVRYWwI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Transformer(nn.Module):\n",
        "    def __init__(self, num_token, max_seq_len, dim_model, d_k=64, d_v=64, n_head=8, dim_hidden=2048, d_prob=0.1, n_dec_layer=6):\n",
        "        super(Transformer, self).__init__()\n",
        "\n",
        "        self.num_token = num_token\n",
        "        self.max_seq_len = max_seq_len\n",
        "        self.embed = Embedding_Layer(num_token=num_token, dim_model=dim_model, max_seq_len=max_seq_len, d_prob=d_prob)\n",
        "        self.decoder = Decoder(dim_model, d_k, d_v, n_head, dim_hidden, d_prob, n_dec_layer)\n",
        "        self.linear = nn.Linear(dim_model, num_token)\n",
        "        self.logsoftmax = nn.LogSoftmax(dim=-1)\n",
        "\n",
        "    def forward(self, src):\n",
        "        positional_encoded_src = self.embed(src) # [128, 20, 512]\n",
        "        # return positional_encoded_src\n",
        "        dec_output = self.decoder(positional_encoded_src)\n",
        "        outputs = self.logsoftmax(self.linear(dec_output))\n",
        "        return outputs\n"
      ],
      "metadata": {
        "id": "H-n6CZXFYa76"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch = 4\n",
        "voc_size = 2\n",
        "max_len =10\n",
        "x = torch.ones(batch, max_len).long()\n",
        "model = Transformer(voc_size, max_len, 32, 8, 8, 4, 128, 0, 2)\n",
        "y = model(x)\n",
        "y.shape # batch, max_len, voc_size"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yPtIegQlgQTV",
        "outputId": "7400c251-6854-4ccb-a30b-39ce40875ced"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([4, 10, 2])"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training Transformer"
      ],
      "metadata": {
        "id": "9W5W6dRSYeWd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(dataloader, epochs, model, criterion, vocab, i2w):\n",
        "\tmodel.train()\n",
        "\tmodel.zero_grad()\n",
        "\toptimizer = Adam(model.parameters(), lr=lr, weight_decay=weight_decay, betas=(beta1, beta2), eps=eps)\n",
        "\tcorrect = 0\n",
        "\n",
        "\tcnt = 0\n",
        "\ttotal_score = 0.\n",
        "\tglobal_step = 0\n",
        "\ttr_loss = 0.\n",
        "\tfor epoch in range(epochs):\n",
        "\n",
        "\t\tfor idx, (src, tgt) in enumerate(dataloader):\n",
        "\t\t\tsrc, tgt = src.to(device), tgt.to(device) # Russian, English sentences\n",
        "\n",
        "\t\t\tsrc = tgt[:, :-1]\n",
        "\t\t\ttgt = tgt[:, 1:]\n",
        "\t\t\t# print(src.size(), tgt.size())\n",
        "\t\t\t# print(src)\n",
        "\t\t\t# print(tgt)\n",
        "\t\t\t# return 0\n",
        "\t\t\toptimizer.zero_grad()\n",
        "\n",
        "\t\t\toutputs = model(src)\n",
        "\n",
        "\t\t\ttgt = torch.flatten(tgt)\n",
        "\t\t\toutputs = outputs.reshape(len(tgt), -1)\n",
        "\t\t\tloss = criterion(outputs, tgt)\n",
        "\t\t\ttr_loss += loss.item()\n",
        "\n",
        "\t\t\tloss.backward()\n",
        "\t\t\ttorch.nn.utils.clip_grad_norm_(model.parameters(), max_norm)\n",
        "\t\t\toptimizer.step()\n",
        "\t\t\tglobal_step += 1\n",
        "\n",
        "\t\t\tpred = outputs.argmax(dim=1, keepdim=True)\n",
        "\t\t\tpred_acc = pred\n",
        "\t\t\ttgt_acc = tgt\n",
        "\n",
        "\t\t\tcorrect += pred_acc.eq(tgt_acc.view_as(pred_acc)).sum().item()\n",
        "\n",
        "\t\t\tcnt += tgt_acc.shape[0]\n",
        "\t\t\tscore = 0.\n",
        "\n",
        "\t\t\t# with torch.no_grad():\n",
        "\t\t\t# \tpred = pred.reshape(batch_size, max_len, -1).detach().cpu().tolist()\n",
        "\t\t\t# \ttgt = tgt.reshape(batch_size, max_len).detach().cpu().tolist()\n",
        "\t\t\t# \tfor p, t in zip(pred, tgt):\n",
        "\t\t\t# \t\teos_idx = t.index(vocab['[PAD]']) if vocab['[PAD]'] in t else len(t)\n",
        "\t\t\t# \t\tp_seq = [i2w[i[0]] for i in p][:eos_idx]\n",
        "\t\t\t# \t\tt_seq = [i2w[i] for i in t][:eos_idx]\n",
        "\t\t\t# \t\tk = 4 if len(t_seq) > 4 else len(t_seq)\n",
        "\t\t\t# \t\ts = utils.bleu_score(p_seq, t_seq, k=k)\n",
        "\t\t\t# \t\tscore += s\n",
        "\t\t\t# \t\ttotal_score += s\n",
        "\n",
        "\t\t\t# score /= batch_size\n",
        "\n",
        "\t\t\tprint(\"\\r[epoch {:3d}/{:3d}] [batch {:4d}/{:4d}] loss: {:.6f} acc: {:.4f} BLEU: {:.4f})\".format(\n",
        "\t\t\t\tepoch, n_epochs, idx + 1, len(dataloader), loss, correct / cnt, score), end=' ')\n",
        "\n",
        "\ttr_loss /= cnt\n",
        "\ttr_acc = correct / cnt\n",
        "\ttr_score = total_score / len(dataloader.dataset) / epochs\n",
        "\n",
        "\treturn tr_loss, tr_acc, tr_score\n",
        "\n",
        "def eval(dataloader, model, lengths=None):\n",
        "    # Implement th\n",
        "\t\tpass\n"
      ],
      "metadata": {
        "id": "wKiEQ9oKYgiK"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ggOF-zJRtcza",
        "outputId": "543052e3-9c68-4cda-ab92-d0381a3f2b98",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "20\n",
            "[epoch   0/  5] [batch    7/   7] loss: 5.645875 acc: 0.1004 BLEU: 0.0000) "
          ]
        }
      ],
      "source": [
        "utils.set_random_seed(42)\n",
        "dataset_dir = '2023/seminar_transformers/data/wmt15_russian_to_english'\n",
        "# dataset_dir = '../data/wmt16_turkish_to_english'\n",
        "max_len=20\n",
        "print(max_len)\n",
        "trn_dataset = dataloader.MiniWMT15_en_ru_Dataset(max_len=20, src_filepath=f'{dataset_dir}/src_train.txt', tgt_filepath=f'{dataset_dir}/tgt_train.txt', vocab=(None, None), is_src=True, is_tgt=False, is_train=True)\n",
        "test_dataset = dataloader.MiniWMT15_en_ru_Dataset(max_len=20, src_filepath=f'{dataset_dir}/src_test.txt', tgt_filepath=None, vocab=(trn_dataset.vocab, None), is_src=True, is_tgt=False, is_train=False)\n",
        "vocab = trn_dataset.vocab\n",
        "\n",
        "i2w = {v: k for k, v in vocab.items()}\n",
        "trn_dataloader = DataLoader(trn_dataset, batch_size=128, shuffle=True, drop_last=True, num_workers=2)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=128, shuffle=False, drop_last=False, num_workers=2)\n",
        "n_token = len(trn_dataset.vocab)\n",
        "model = Transformer(num_token=n_token, max_seq_len=max_len, dim_model=model_dim, d_k=d_k, d_v=d_v, n_head=n_head, dim_hidden=hidden_size, d_prob=d_prob, n_dec_layer=num_dec_layers)\n",
        "model = model.to(device)\n",
        "criterion = nn.NLLLoss(ignore_index=vocab['[PAD]'])\n",
        "\n",
        "n_epochs=5\n",
        "tr_loss, tr_acc, tr_score = train(trn_dataloader, n_epochs, model, criterion, vocab, i2w)\n",
        "# tr_loss, tr_acc, tr_score = train_debug(trn_dataloader, 1, model, criterion, vocab, i2w)\n",
        "# print(\"tr: ({:.4f}, {:5.2f}, {:5.2f}) | \".format(tr_loss, tr_acc * 100, tr_score * 100), end='')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_sentence():\n",
        "  input_text = torch.zeros(1,1).long()\n",
        "\n",
        "  output_text = []\n",
        "  for i in range(20):\n",
        "    outputs = model(input_text)\n",
        "    pred = outputs[:, -1:].argmax(dim=-1)\n",
        "    input_text = torch.concat([input_text, pred], dim=1)\n",
        "\n",
        "    output_text.append(pred)\n",
        "  return \" \".join([i2w[i] for i in input_text.flatten().tolist()])\n",
        "generate_sentence()"
      ],
      "metadata": {
        "id": "rHNTm7c86I8X"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3.9.13 ('dlub_2022')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "c0020dc2bff844068f9b5b1ead3ea97b8c4f02721b75096722229b3552c22662"
      }
    },
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}